{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing INTERNET speeds in Morocco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm  # progress bar in Jupyter\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download data\n",
    "\n",
    "First, download OOKLA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_start(year: int, q: int) -> datetime:\n",
    "    if not 1 <= q <= 4:\n",
    "        raise ValueError(\"Quarter must be within [1, 2, 3, 4]\")\n",
    "    month = [1, 4, 7, 10]\n",
    "    return datetime(year, month[q - 1], 1)\n",
    "\n",
    "def quarter_start(year: int, q: int) -> datetime:\n",
    "    if not 1 <= q <= 4:\n",
    "        raise ValueError(\"Quarter must be within [1, 2, 3, 4]\")\n",
    "    month = [1, 4, 7, 10]\n",
    "    return datetime(year, month[q - 1], 1)\n",
    "\n",
    "def quarter_end(year: int, q: int) -> datetime:\n",
    "    if q == 4:\n",
    "        return datetime(year + 1, 1, 1)\n",
    "    return quarter_start(year, q + 1)\n",
    "\n",
    "def get_tile_url(service_type: str, year: int, q: int) -> str | None:\n",
    "    dt = quarter_start(year, q)\n",
    "    end_dt = quarter_end(year, q)\n",
    "    now = datetime.utcnow()\n",
    "\n",
    "    if now < end_dt:\n",
    "        print(f\"‚è© Skipping {service_type} {year} Q{q} (quarter not yet complete)\")\n",
    "        return None\n",
    "\n",
    "    base_url = \"https://ookla-open-data.s3-us-west-2.amazonaws.com/shapefiles/performance\"\n",
    "    url = f\"{base_url}/type%3D{service_type}/year%3D{dt:%Y}/quarter%3D{q}/{dt:%Y-%m-%d}_performance_{service_type}_tiles.zip\"\n",
    "    return url\n",
    "\n",
    "def download_file_with_progress(url: str, output_dir: str = \"data\") -> str:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    local_filename = os.path.join(output_dir, url.split(\"/\")[-1])\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "\n",
    "    t = tqdm(total=total_size, unit='iB', unit_scale=True, desc=f\"Downloading {os.path.basename(local_filename)}\")\n",
    "\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for data in response.iter_content(block_size):\n",
    "            t.update(len(data))\n",
    "            f.write(data)\n",
    "\n",
    "    t.close()\n",
    "\n",
    "    if total_size != 0 and t.n != total_size:\n",
    "        print(\"‚ö†Ô∏è WARNING: Download might be incomplete.\")\n",
    "    \n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure ctype,years,and quarters as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è© Skipping fixed 2025 Q2 (quarter not yet complete)\n",
      "‚è© Skipping fixed 2025 Q3 (quarter not yet complete)\n",
      "‚è© Skipping fixed 2025 Q4 (quarter not yet complete)\n",
      "‚è© Skipping mobile 2025 Q2 (quarter not yet complete)\n",
      "‚è© Skipping mobile 2025 Q3 (quarter not yet complete)\n",
      "‚è© Skipping mobile 2025 Q4 (quarter not yet complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_145205/3529908371.py:21: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "ctype = [\"fixed\",\"mobile\"]\n",
    "years = [2023,2025]\n",
    "quarters = [1, 2, 3, 4]\n",
    "for t in ctype :\n",
    "    for year in years :\n",
    "        for q in quarters :\n",
    "            url = get_tile_url(t, year, q)\n",
    "           # download_file_with_progress(url) #uncomment to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Morocco = gp.read_file(\"data/Morocco/morocco.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77c05028e0043fe900ec26e69876fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üì• Loading tiles:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2025-01-01_performance_fixed_tiles.zip: 6364159 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2025-01-01_performance_fixed_tiles_morocco.shp (18874 features)\n",
      "‚úÖ 2024-07-01_performance_mobile_tiles.zip: 3773658 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-07-01_performance_mobile_tiles_morocco.shp (12335 features)\n",
      "‚úÖ 2024-01-01_performance_mobile_tiles.zip: 3674000 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-01-01_performance_mobile_tiles_morocco.shp (12563 features)\n",
      "‚úÖ 2025-01-01_performance_mobile_tiles.zip: 3388115 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2025-01-01_performance_mobile_tiles_morocco.shp (12002 features)\n",
      "‚úÖ 2024-10-01_performance_fixed_tiles.zip: 6561086 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-10-01_performance_fixed_tiles_morocco.shp (18414 features)\n",
      "‚úÖ 2024-04-01_performance_fixed_tiles.zip: 6492072 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-04-01_performance_fixed_tiles_morocco.shp (18484 features)\n",
      "‚úÖ 2024-01-01_performance_fixed_tiles.zip: 6655986 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-01-01_performance_fixed_tiles_morocco.shp (19127 features)\n",
      "‚úÖ 2024-07-01_performance_fixed_tiles.zip: 6655377 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-07-01_performance_fixed_tiles_morocco.shp (18587 features)\n",
      "‚úÖ 2024-04-01_performance_mobile_tiles.zip: 3703161 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-04-01_performance_mobile_tiles_morocco.shp (13523 features)\n",
      "‚úÖ 2024-10-01_performance_mobile_tiles.zip: 3551267 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-10-01_performance_mobile_tiles_morocco.shp (12340 features)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List all matching zip files\n",
    "zip_files = [f for f in os.listdir(\"data\") if f.endswith(\".zip\") and \"tiles\" in f]\n",
    "\n",
    "# Progress bar for loading\n",
    "for filename in tqdm(zip_files, desc=\"Loading tiles\"):\n",
    "    path = os.path.join(\"data\", filename)\n",
    "    try:\n",
    "        gdf = gp.read_file(path)\n",
    "        print(f\"LOADED : {filename}: {len(gdf)} features\")\n",
    "        name = filename\n",
    "        gdf = gdf.to_crs(Morocco.crs)\n",
    "        morocco_tiles = gp.clip(gdf, Morocco)\n",
    "        output_name = name.replace(\".zip\", \"_morocco.shp\")\n",
    "        output_path = os.path.join(\"MoroccoData\", output_name)\n",
    "        morocco_tiles.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "        print(f\"SAVED: {output_path} ({len(morocco_tiles)} features)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {filename}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracted from link[ https://github.com/teamookla/ookla-open-data/edit/master/README.md ]\n",
    "\n",
    "#### Tile Attributes\n",
    "Each tile contains the following adjoining attributes:\n",
    "\n",
    "| Field Name        | Type        | Description                                                                                                                             | Notes                                                                                                                                                                              |\n",
    "|-------------------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `avg_d_kbps`      | Integer     | The average download speed of all tests performed in the tile, represented in kilobits per second.                                      |                                                                                                                                                                                    |\n",
    "| `avg_u_kbps`      | Integer     | The average upload speed of all tests performed in the tile, represented in kilobits per second.                                        |                                                                                                                                                                                    |\n",
    "| `avg_lat_ms`      | Integer     | The average latency of all tests performed in the tile, represented in milliseconds                                                     |                                                                                                                                                                                    |\n",
    "| `avg_lat_down_ms` | Integer     | The average latency under load of all tests performed in the tile as measured during the download phase of the test. Represented in ms. | Parquet-only. Added 2023-02-23 beginning in Q4 2022 dataset. This column is sparsely populated-- some rows will have a null value as not all versions of Speedtest can perform this measurement. |\n",
    "| `avg_lat_up_ms`   | Integer     | The average latency under load of all tests performed in the tile as measured during the upload phase of the test. Represented in ms.   | Parquet-only. Added 2023-02-23 beginning in Q4 2022 dataset. This column is sparsely populated-- some rows will have a null value as not all versions of Speedtest can perform this measurement. |\n",
    "| `tests`           | Integer     | The number of tests taken in the tile. |\n",
    "| `devices`         | Integer     | The number of unique devices contributing tests in the tile. |\n",
    "| `quadkey`         | Text        | The quadkey representing the tile.  |\n",
    "| `tile_x`\t\t\t| Numeric\t  | X coordinate of the tile's centroid.| Added 2023-07-01 beginning in the Q3 2023 dataset.\n",
    "| `tile_y`          | Numeric     | Y coordinate of the tile's centroid.| Added 2023-07-01 beginning in the Q3 2023 dataset.\n",
    "\n",
    "\n",
    "#### Quadkeys\n",
    "\n",
    "[Quadkeys](https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system) can act as a unique identifier for the tile. This can be useful for joining data spatially from multiple periods (quarters), creating coarser spatial aggregations without using geospatial functions, spatial indexing, partitioning, and an alternative for storing and deriving the tile geometry.\n",
    "\n",
    "#### Layers\n",
    "Two layers are distributed as separate sets of files:\n",
    "\n",
    "* `performance_mobile_tiles` - Tiles containing tests taken from mobile devices with GPS-quality location and a cellular connection type (e.g. 4G LTE, 5G NR).\n",
    "* `performance_fixed_tiles` - Tiles containing tests taken from mobile devices with GPS-quality location and a non-cellular connection type (e.g. WiFi, ethernet).\n",
    "\n",
    "#### Time Period and Update Frequency\n",
    "\n",
    "Layers are generated based on a quarter year of data (three months) and files will be updated and added on a quarterly basis. A `/year=2020/quarter=1/` period, the first quarter of the year 2020, would include all data generated on or after `2020-01-01` and before `2020-04-01`.\n",
    "\n",
    "Data is subject to be reaggregated regularly in order to honor Data Subject Access Requests (DSAR) as is applicable in certain jurisdictions under laws including but not limited to General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), and Lei Geral de Prote√ß√£o de Dados (LGPD). Therefore, data accessed at different times may result in variation in the total number of tests, tiles, and resulting performance metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MOROCCO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b0deff1ad64e619877bd3733b66107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Morocco tiles:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED : 2024-04-01_performance_fixed_tiles_morocco.shp: 18484 features\n",
      "LOADED : 2024-07-01_performance_fixed_tiles_morocco.shp: 18587 features\n",
      "LOADED : 2024-10-01_performance_fixed_tiles_morocco.shp: 18414 features\n",
      "LOADED : 2024-04-01_performance_mobile_tiles_morocco.shp: 13523 features\n",
      "LOADED : 2024-01-01_performance_fixed_tiles_morocco.shp: 19127 features\n",
      "LOADED : 2024-07-01_performance_mobile_tiles_morocco.shp: 12335 features\n",
      "LOADED : 2025-01-01_performance_fixed_tiles_morocco.shp: 18874 features\n",
      "LOADED : 2024-10-01_performance_mobile_tiles_morocco.shp: 12340 features\n",
      "LOADED : 2025-01-01_performance_mobile_tiles_morocco.shp: 12002 features\n",
      "LOADED : 2024-01-01_performance_mobile_tiles_morocco.shp: 12563 features\n"
     ]
    },
    {
     "ename": "DataSourceError",
     "evalue": "sqlite3_open(ResultData/all_quarters_combined.gpkg) failed: unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCPLE_OpenFailedError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1947\u001b[39m, in \u001b[36mpyogrio._io.ogr_create\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_err.pyx:183\u001b[39m, in \u001b[36mpyogrio._err.exc_wrap_pointer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCPLE_OpenFailedError\u001b[39m: sqlite3_open(ResultData/all_quarters_combined.gpkg) failed: unable to open database file",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mDataSourceError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# At this point, final_gdf is your ‚Äúwide‚Äù GeoDataFrame with columns like:\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m#   quadkey, geometry, 2024-01-01_fixed_avg_d_kbps, 2024-01-01_fixed_avg_u_kbps, etc.\u001b[39;00m\n\u001b[32m     61\u001b[39m \n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Save as a GeoPackage instead of ESRI Shapefile so you don‚Äôt lose column names:\u001b[39;00m\n\u001b[32m     63\u001b[39m final_out = \u001b[33m\"\u001b[39m\u001b[33mall_quarters_combined.gpkg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mfinal_gdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResultData\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGPKG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Saved combined dataset to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# -----------------------------------------\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Create a second file that has differences \u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# between consecutive quarters for each type\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ENSIAS/S4/DATA DRIVEN DECISION MAKING/jupyter/lib/python3.12/site-packages/geopandas/geodataframe.py:1536\u001b[39m, in \u001b[36mGeoDataFrame.to_file\u001b[39m\u001b[34m(self, filename, driver, schema, index, **kwargs)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \n\u001b[32m   1443\u001b[39m \u001b[33;03mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1532\u001b[39m \n\u001b[32m   1533\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_file\n\u001b[32m-> \u001b[39m\u001b[32m1536\u001b[39m \u001b[43m_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ENSIAS/S4/DATA DRIVEN DECISION MAKING/jupyter/lib/python3.12/site-packages/geopandas/io/file.py:686\u001b[39m, in \u001b[36m_to_file\u001b[39m\u001b[34m(df, filename, driver, schema, index, mode, crs, engine, metadata, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m should be one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, got \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m     \u001b[43m_to_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    688\u001b[39m     _to_file_fiona(df, filename, driver, schema, crs, mode, metadata, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ENSIAS/S4/DATA DRIVEN DECISION MAKING/jupyter/lib/python3.12/site-packages/geopandas/io/file.py:748\u001b[39m, in \u001b[36m_to_file_pyogrio\u001b[39m\u001b[34m(df, filename, driver, schema, crs, mode, metadata, **kwargs)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df.columns.is_unique:\n\u001b[32m    746\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGeoDataFrame cannot contain duplicated column names.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ENSIAS/S4/DATA DRIVEN DECISION MAKING/jupyter/lib/python3.12/site-packages/pyogrio/geopandas.py:662\u001b[39m, in \u001b[36mwrite_dataframe\u001b[39m\u001b[34m(df, path, layer, driver, encoding, geometry_type, promote_to_multi, nan_as_null, append, use_arrow, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m geometry_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    660\u001b[39m     geometry = to_wkb(geometry.values)\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ENSIAS/S4/DATA DRIVEN DECISION MAKING/jupyter/lib/python3.12/site-packages/pyogrio/raw.py:723\u001b[39m, in \u001b[36mwrite\u001b[39m\u001b[34m(path, geometry, field_data, fields, field_mask, layer, driver, geometry_type, crs, encoding, promote_to_multi, nan_as_null, append, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, gdal_tz_offsets, **kwargs)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;66;03m# preprocess kwargs and split in dataset and layer creation options\u001b[39;00m\n\u001b[32m    719\u001b[39m dataset_kwargs, layer_kwargs = _preprocess_options_kwargs(\n\u001b[32m    720\u001b[39m     driver, dataset_options, layer_options, kwargs\n\u001b[32m    721\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m \u001b[43mogr_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:2307\u001b[39m, in \u001b[36mpyogrio._io.ogr_write\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:2135\u001b[39m, in \u001b[36mpyogrio._io.create_ogr_dataset_layer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1956\u001b[39m, in \u001b[36mpyogrio._io.ogr_create\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDataSourceError\u001b[39m: sqlite3_open(ResultData/all_quarters_combined.gpkg) failed: unable to open database file"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_FOLDER = \"MoroccoData\"\n",
    "\n",
    "# Grab only shapefiles that end with \"_morocco.shp\"\n",
    "Morocco_Data_files = [\n",
    "    f for f in os.listdir(DATA_FOLDER) \n",
    "    if f.endswith(\"_morocco.shp\")\n",
    "]\n",
    "\n",
    "# This will accumulate your ‚Äúwide‚Äù data\n",
    "final_gdf = None\n",
    "\n",
    "# Regex to capture:\n",
    "#   2024-01-01_performance_fixed_tiles_morocco.shp\n",
    "#   group(1) => 2024-01-01\n",
    "#   group(2) => fixed or mobile\n",
    "filename_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2})_.*?(fixed|mobile).*?_morocco\\.shp')\n",
    "\n",
    "for filename in tqdm(Morocco_Data_files, desc=\"Loading Morocco tiles\"):\n",
    "    path = os.path.join(DATA_FOLDER, filename)\n",
    "    try:\n",
    "        gdf = gp.read_file(path)\n",
    "        print(f\"LOADED : {filename}: {len(gdf)} features\")\n",
    "\n",
    "        # Extract quarter/date and type from filename\n",
    "        match = filename_pattern.search(filename)\n",
    "        if not match:\n",
    "            print(f\"‚ùå Could not parse quarter/type from {filename}\")\n",
    "            continue\n",
    "        \n",
    "        quarter_str = match.group(1)  # e.g. \"2024-01-01\"\n",
    "        conn_type  = match.group(2)   # e.g. \"fixed\" or \"mobile\"\n",
    "        \n",
    "        # Build a rename map. We keep 'quadkey' and 'geometry' the same\n",
    "        rename_map = {}\n",
    "        for col in gdf.columns:\n",
    "            if col not in ['quadkey', 'geometry']:\n",
    "                # Prepend <quarter>_<type>_\n",
    "                new_col = f\"{quarter_str}_{conn_type}_{col}\"\n",
    "                rename_map[col] = new_col\n",
    "        \n",
    "        # Rename columns\n",
    "        gdf_renamed = gdf.rename(columns=rename_map)\n",
    "        \n",
    "        # If final_gdf is None, just store this one.\n",
    "        if final_gdf is None:\n",
    "            final_gdf = gdf_renamed\n",
    "        else:\n",
    "            # We'll do a full outer merge only on 'quadkey'\n",
    "            # Because tile_x & tile_y do not exist in your shapefiles\n",
    "            gdf_no_geom = gdf_renamed.drop(columns=['geometry'])\n",
    "            final_gdf = final_gdf.merge(\n",
    "                gdf_no_geom,\n",
    "                on='quadkey',\n",
    "                how='outer'\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {filename}: {e}\")\n",
    "\n",
    "# At this point, final_gdf is your ‚Äúwide‚Äù GeoDataFrame with columns like:\n",
    "#   quadkey, geometry, 2024-01-01_fixed_avg_d_kbps, 2024-01-01_fixed_avg_u_kbps, etc.\n",
    "\n",
    "# Save as a GeoPackage instead of ESRI Shapefile so you don‚Äôt lose column names:\n",
    "final_out = \"all_quarters_combined.gpkg\"\n",
    "final_gdf.to_file(final_out, driver=\"GPKG\")\n",
    "print(f\"‚úÖ Saved combined dataset to: {final_out}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Create a second file that has differences \n",
    "# between consecutive quarters for each type\n",
    "import pandas as pd\n",
    "\n",
    "diff_gdf = final_gdf.copy()\n",
    "\n",
    "# We‚Äôll parse all columns that match e.g. 2024-01-01_fixed_avg_d_kbps\n",
    "col_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2})_(fixed|mobile)_(.*)')\n",
    "\n",
    "# Build a structure to group columns by (type, metric), e.g. (fixed, avg_d_kbps).\n",
    "quarter_type_to_metrics = {}\n",
    "\n",
    "for col in diff_gdf.columns:\n",
    "    m = col_pattern.match(col)\n",
    "    if m:\n",
    "        q_str = m.group(1)\n",
    "        t_str = m.group(2)\n",
    "        metric_name = m.group(3)\n",
    "        \n",
    "        # Keep track of columns by quarter\n",
    "        quarter_type_to_metrics.setdefault((t_str, metric_name), []).append(q_str)\n",
    "\n",
    "# For each (type, metric), sort quarters & build difference columns\n",
    "for (conn_type, metric_name), quarter_list in quarter_type_to_metrics.items():\n",
    "    quarter_list_sorted = sorted(quarter_list)\n",
    "    for i in range(1, len(quarter_list_sorted)):\n",
    "        prev_q = quarter_list_sorted[i-1]\n",
    "        curr_q = quarter_list_sorted[i]\n",
    "        \n",
    "        prev_col = f\"{prev_q}_{conn_type}_{metric_name}\"\n",
    "        curr_col = f\"{curr_q}_{conn_type}_{metric_name}\"\n",
    "        \n",
    "        # If for some reason we‚Äôre missing a column, skip\n",
    "        if prev_col not in diff_gdf.columns or curr_col not in diff_gdf.columns or metric_name in ['tests', 'devices']:\n",
    "            continue\n",
    "        \n",
    "        diff_col_name = f\"D_{curr_q}_M_{prev_q}_{conn_type}_{metric_name}\"\n",
    "        diff_gdf[diff_col_name] = diff_gdf[curr_col] - diff_gdf[prev_col]\n",
    "\n",
    "# Save differences as well, again in a GeoPackage\n",
    "diff_out = \"all_quarters_differences.gpkg\"\n",
    "diff_gdf.to_file(diff_out, driver=\"GPKG\")\n",
    "print(f\"‚úÖ Saved differences dataset to: {diff_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
