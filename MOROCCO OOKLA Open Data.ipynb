{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing INTERNET speeds in Morocco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm  # progress bar in Jupyter\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download data\n",
    "\n",
    "First, download OOKLA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_start(year: int, q: int) -> datetime:\n",
    "    if not 1 <= q <= 4:\n",
    "        raise ValueError(\"Quarter must be within [1, 2, 3, 4]\")\n",
    "    month = [1, 4, 7, 10]\n",
    "    return datetime(year, month[q - 1], 1)\n",
    "\n",
    "def quarter_start(year: int, q: int) -> datetime:\n",
    "    if not 1 <= q <= 4:\n",
    "        raise ValueError(\"Quarter must be within [1, 2, 3, 4]\")\n",
    "    month = [1, 4, 7, 10]\n",
    "    return datetime(year, month[q - 1], 1)\n",
    "\n",
    "def quarter_end(year: int, q: int) -> datetime:\n",
    "    if q == 4:\n",
    "        return datetime(year + 1, 1, 1)\n",
    "    return quarter_start(year, q + 1)\n",
    "\n",
    "def get_tile_url(service_type: str, year: int, q: int) -> str | None:\n",
    "    dt = quarter_start(year, q)\n",
    "    end_dt = quarter_end(year, q)\n",
    "    now = datetime.utcnow()\n",
    "\n",
    "    if now < end_dt:\n",
    "        print(f\"‚è© Skipping {service_type} {year} Q{q} (quarter not yet complete)\")\n",
    "        return None\n",
    "\n",
    "    base_url = \"https://ookla-open-data.s3-us-west-2.amazonaws.com/shapefiles/performance\"\n",
    "    url = f\"{base_url}/type%3D{service_type}/year%3D{dt:%Y}/quarter%3D{q}/{dt:%Y-%m-%d}_performance_{service_type}_tiles.zip\"\n",
    "    return url\n",
    "\n",
    "def download_file_with_progress(url: str, output_dir: str = \"data\") -> str:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    local_filename = os.path.join(output_dir, url.split(\"/\")[-1])\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "\n",
    "    t = tqdm(total=total_size, unit='iB', unit_scale=True, desc=f\"Downloading {os.path.basename(local_filename)}\")\n",
    "\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for data in response.iter_content(block_size):\n",
    "            t.update(len(data))\n",
    "            f.write(data)\n",
    "\n",
    "    t.close()\n",
    "\n",
    "    if total_size != 0 and t.n != total_size:\n",
    "        print(\"‚ö†Ô∏è WARNING: Download might be incomplete.\")\n",
    "    \n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure ctype,years,and quarters as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è© Skipping fixed 2025 Q2 (quarter not yet complete)\n",
      "‚è© Skipping fixed 2025 Q3 (quarter not yet complete)\n",
      "‚è© Skipping fixed 2025 Q4 (quarter not yet complete)\n",
      "‚è© Skipping mobile 2025 Q2 (quarter not yet complete)\n",
      "‚è© Skipping mobile 2025 Q3 (quarter not yet complete)\n",
      "‚è© Skipping mobile 2025 Q4 (quarter not yet complete)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_145205/3529908371.py:21: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "ctype = [\"fixed\",\"mobile\"]\n",
    "years = [2023,2025]\n",
    "quarters = [1, 2, 3, 4]\n",
    "for t in ctype :\n",
    "    for year in years :\n",
    "        for q in quarters :\n",
    "            url = get_tile_url(t, year, q)\n",
    "           # download_file_with_progress(url) #uncomment to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Morocco = gp.read_file(\"data/Morocco/morocco.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77c05028e0043fe900ec26e69876fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üì• Loading tiles:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2025-01-01_performance_fixed_tiles.zip: 6364159 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2025-01-01_performance_fixed_tiles_morocco.shp (18874 features)\n",
      "‚úÖ 2024-07-01_performance_mobile_tiles.zip: 3773658 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-07-01_performance_mobile_tiles_morocco.shp (12335 features)\n",
      "‚úÖ 2024-01-01_performance_mobile_tiles.zip: 3674000 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-01-01_performance_mobile_tiles_morocco.shp (12563 features)\n",
      "‚úÖ 2025-01-01_performance_mobile_tiles.zip: 3388115 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2025-01-01_performance_mobile_tiles_morocco.shp (12002 features)\n",
      "‚úÖ 2024-10-01_performance_fixed_tiles.zip: 6561086 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-10-01_performance_fixed_tiles_morocco.shp (18414 features)\n",
      "‚úÖ 2024-04-01_performance_fixed_tiles.zip: 6492072 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-04-01_performance_fixed_tiles_morocco.shp (18484 features)\n",
      "‚úÖ 2024-01-01_performance_fixed_tiles.zip: 6655986 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-01-01_performance_fixed_tiles_morocco.shp (19127 features)\n",
      "‚úÖ 2024-07-01_performance_fixed_tiles.zip: 6655377 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-07-01_performance_fixed_tiles_morocco.shp (18587 features)\n",
      "‚úÖ 2024-04-01_performance_mobile_tiles.zip: 3703161 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-04-01_performance_mobile_tiles_morocco.shp (13523 features)\n",
      "‚úÖ 2024-10-01_performance_mobile_tiles.zip: 3551267 features | CRS: EPSG:4326 | Columns: ['quadkey', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices', 'geometry']\n",
      "‚úÖ Saved: MoroccoData/2024-10-01_performance_mobile_tiles_morocco.shp (12340 features)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List all matching zip files\n",
    "zip_files = [f for f in os.listdir(\"data\") if f.endswith(\".zip\") and \"tiles\" in f]\n",
    "\n",
    "# Progress bar for loading\n",
    "for filename in tqdm(zip_files, desc=\"Loading tiles\"):\n",
    "    path = os.path.join(\"data\", filename)\n",
    "    try:\n",
    "        gdf = gp.read_file(path)\n",
    "        print(f\"LOADED : {filename}: {len(gdf)} features\")\n",
    "        name = filename\n",
    "        gdf = gdf.to_crs(Morocco.crs)\n",
    "        morocco_tiles = gp.clip(gdf, Morocco)\n",
    "        output_name = name.replace(\".zip\", \"_morocco.shp\")\n",
    "        output_path = os.path.join(\"MoroccoData\", output_name)\n",
    "        morocco_tiles.to_file(output_path, driver=\"ESRI Shapefile\")\n",
    "        print(f\"SAVED: {output_path} ({len(morocco_tiles)} features)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {filename}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracted from link[ https://github.com/teamookla/ookla-open-data/edit/master/README.md ]\n",
    "\n",
    "#### Tile Attributes\n",
    "Each tile contains the following adjoining attributes:\n",
    "\n",
    "| Field Name        | Type        | Description                                                                                                                             | Notes                                                                                                                                                                              |\n",
    "|-------------------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `avg_d_kbps`      | Integer     | The average download speed of all tests performed in the tile, represented in kilobits per second.                                      |                                                                                                                                                                                    |\n",
    "| `avg_u_kbps`      | Integer     | The average upload speed of all tests performed in the tile, represented in kilobits per second.                                        |                                                                                                                                                                                    |\n",
    "| `avg_lat_ms`      | Integer     | The average latency of all tests performed in the tile, represented in milliseconds                                                     |                                                                                                                                                                                    |\n",
    "| `avg_lat_down_ms` | Integer     | The average latency under load of all tests performed in the tile as measured during the download phase of the test. Represented in ms. | Parquet-only. Added 2023-02-23 beginning in Q4 2022 dataset. This column is sparsely populated-- some rows will have a null value as not all versions of Speedtest can perform this measurement. |\n",
    "| `avg_lat_up_ms`   | Integer     | The average latency under load of all tests performed in the tile as measured during the upload phase of the test. Represented in ms.   | Parquet-only. Added 2023-02-23 beginning in Q4 2022 dataset. This column is sparsely populated-- some rows will have a null value as not all versions of Speedtest can perform this measurement. |\n",
    "| `tests`           | Integer     | The number of tests taken in the tile. |\n",
    "| `devices`         | Integer     | The number of unique devices contributing tests in the tile. |\n",
    "| `quadkey`         | Text        | The quadkey representing the tile.  |\n",
    "| `tile_x`\t\t\t| Numeric\t  | X coordinate of the tile's centroid.| Added 2023-07-01 beginning in the Q3 2023 dataset.\n",
    "| `tile_y`          | Numeric     | Y coordinate of the tile's centroid.| Added 2023-07-01 beginning in the Q3 2023 dataset.\n",
    "\n",
    "\n",
    "#### Quadkeys\n",
    "\n",
    "[Quadkeys](https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system) can act as a unique identifier for the tile. This can be useful for joining data spatially from multiple periods (quarters), creating coarser spatial aggregations without using geospatial functions, spatial indexing, partitioning, and an alternative for storing and deriving the tile geometry.\n",
    "\n",
    "#### Layers\n",
    "Two layers are distributed as separate sets of files:\n",
    "\n",
    "* `performance_mobile_tiles` - Tiles containing tests taken from mobile devices with GPS-quality location and a cellular connection type (e.g. 4G LTE, 5G NR).\n",
    "* `performance_fixed_tiles` - Tiles containing tests taken from mobile devices with GPS-quality location and a non-cellular connection type (e.g. WiFi, ethernet).\n",
    "\n",
    "#### Time Period and Update Frequency\n",
    "\n",
    "Layers are generated based on a quarter year of data (three months) and files will be updated and added on a quarterly basis. A `/year=2020/quarter=1/` period, the first quarter of the year 2020, would include all data generated on or after `2020-01-01` and before `2020-04-01`.\n",
    "\n",
    "Data is subject to be reaggregated regularly in order to honor Data Subject Access Requests (DSAR) as is applicable in certain jurisdictions under laws including but not limited to General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), and Lei Geral de Prote√ß√£o de Dados (LGPD). Therefore, data accessed at different times may result in variation in the total number of tests, tiles, and resulting performance metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MOROCCO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab40ef2ba94d4bf68d811e46f7f9fab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Morocco tiles:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED : 2024-04-01_performance_fixed_tiles_morocco.shp: 18484 features\n",
      "LOADED : 2024-07-01_performance_fixed_tiles_morocco.shp: 18587 features\n",
      "LOADED : 2024-10-01_performance_fixed_tiles_morocco.shp: 18414 features\n",
      "LOADED : 2024-04-01_performance_mobile_tiles_morocco.shp: 13523 features\n",
      "LOADED : 2024-01-01_performance_fixed_tiles_morocco.shp: 19127 features\n",
      "LOADED : 2024-07-01_performance_mobile_tiles_morocco.shp: 12335 features\n",
      "LOADED : 2025-01-01_performance_fixed_tiles_morocco.shp: 18874 features\n",
      "LOADED : 2024-10-01_performance_mobile_tiles_morocco.shp: 12340 features\n",
      "LOADED : 2025-01-01_performance_mobile_tiles_morocco.shp: 12002 features\n",
      "LOADED : 2024-01-01_performance_mobile_tiles_morocco.shp: 12563 features\n",
      "‚úÖ Saved combined dataset to: all_quarters_combined.gpkg\n",
      "‚úÖ Saved differences dataset to: all_quarters_differences.gpkg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_FOLDER = \"MoroccoData\"\n",
    "\n",
    "# Grab only shapefiles that end with \"_morocco.shp\"\n",
    "Morocco_Data_files = [\n",
    "    f for f in os.listdir(DATA_FOLDER) \n",
    "    if f.endswith(\"_morocco.shp\")\n",
    "]\n",
    "\n",
    "# This will accumulate your ‚Äúwide‚Äù data\n",
    "final_gdf = None\n",
    "\n",
    "# Regex to capture:\n",
    "#   2024-01-01_performance_fixed_tiles_morocco.shp\n",
    "#   group(1) => 2024-01-01\n",
    "#   group(2) => fixed or mobile\n",
    "filename_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2})_.*?(fixed|mobile).*?_morocco\\.shp')\n",
    "\n",
    "for filename in tqdm(Morocco_Data_files, desc=\"Loading Morocco tiles\"):\n",
    "    path = os.path.join(DATA_FOLDER, filename)\n",
    "    try:\n",
    "        gdf = gp.read_file(path)\n",
    "        print(f\"LOADED : {filename}: {len(gdf)} features\")\n",
    "\n",
    "        # Extract quarter/date and type from filename\n",
    "        match = filename_pattern.search(filename)\n",
    "        if not match:\n",
    "            print(f\"‚ùå Could not parse quarter/type from {filename}\")\n",
    "            continue\n",
    "        \n",
    "        quarter_str = match.group(1)  # e.g. \"2024-01-01\"\n",
    "        conn_type  = match.group(2)   # e.g. \"fixed\" or \"mobile\"\n",
    "        \n",
    "        # Build a rename map. We keep 'quadkey' and 'geometry' the same\n",
    "        rename_map = {}\n",
    "        for col in gdf.columns:\n",
    "            if col not in ['quadkey', 'geometry']:\n",
    "                # Prepend <quarter>_<type>_\n",
    "                new_col = f\"{quarter_str}_{conn_type}_{col}\"\n",
    "                rename_map[col] = new_col\n",
    "        \n",
    "        # Rename columns\n",
    "        gdf_renamed = gdf.rename(columns=rename_map)\n",
    "        \n",
    "        # If final_gdf is None, just store this one.\n",
    "        if final_gdf is None:\n",
    "            final_gdf = gdf_renamed\n",
    "        else:\n",
    "            # We'll do a full outer merge only on 'quadkey'\n",
    "            # Because tile_x & tile_y do not exist in your shapefiles\n",
    "            gdf_no_geom = gdf_renamed.drop(columns=['geometry'])\n",
    "            final_gdf = final_gdf.merge(\n",
    "                gdf_no_geom,\n",
    "                on='quadkey',\n",
    "                how='outer'\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {filename}: {e}\")\n",
    "\n",
    "# At this point, final_gdf is your ‚Äúwide‚Äù GeoDataFrame with columns like:\n",
    "#   quadkey, geometry, 2024-01-01_fixed_avg_d_kbps, 2024-01-01_fixed_avg_u_kbps, etc.\n",
    "\n",
    "# Save as a GeoPackage instead of ESRI Shapefile so you don‚Äôt lose column names:\n",
    "final_out = \"all_quarters_combined.gpkg\"\n",
    "final_gdf.to_file(final_out, driver=\"GPKG\")\n",
    "print(f\"‚úÖ Saved combined dataset to: {final_out}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Create a second file that has differences \n",
    "# between consecutive quarters for each type\n",
    "import pandas as pd\n",
    "\n",
    "diff_gdf = final_gdf.copy()\n",
    "\n",
    "# We‚Äôll parse all columns that match e.g. 2024-01-01_fixed_avg_d_kbps\n",
    "col_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2})_(fixed|mobile)_(.*)')\n",
    "\n",
    "# Build a structure to group columns by (type, metric), e.g. (fixed, avg_d_kbps).\n",
    "quarter_type_to_metrics = {}\n",
    "\n",
    "for col in diff_gdf.columns:\n",
    "    m = col_pattern.match(col)\n",
    "    if m:\n",
    "        q_str = m.group(1)\n",
    "        t_str = m.group(2)\n",
    "        metric_name = m.group(3)\n",
    "        \n",
    "        # Keep track of columns by quarter\n",
    "        quarter_type_to_metrics.setdefault((t_str, metric_name), []).append(q_str)\n",
    "\n",
    "# For each (type, metric), sort quarters & build difference columns\n",
    "for (conn_type, metric_name), quarter_list in quarter_type_to_metrics.items():\n",
    "    quarter_list_sorted = sorted(quarter_list)\n",
    "    for i in range(1, len(quarter_list_sorted)):\n",
    "        prev_q = quarter_list_sorted[i-1]\n",
    "        curr_q = quarter_list_sorted[i]\n",
    "        \n",
    "        prev_col = f\"{prev_q}_{conn_type}_{metric_name}\"\n",
    "        curr_col = f\"{curr_q}_{conn_type}_{metric_name}\"\n",
    "        \n",
    "        # If for some reason we‚Äôre missing a column, skip\n",
    "        if prev_col not in diff_gdf.columns or curr_col not in diff_gdf.columns or metric_name in ['tests', 'devices']:\n",
    "            continue\n",
    "        \n",
    "        diff_col_name = f\"D_{curr_q}_M_{prev_q}_{conn_type}_{metric_name}\"\n",
    "        diff_gdf[diff_col_name] = diff_gdf[curr_col] - diff_gdf[prev_col]\n",
    "\n",
    "# Save differences as well, again in a GeoPackage\n",
    "diff_out = \"all_quarters_differences.gpkg\"\n",
    "diff_gdf.to_file(diff_out, driver=\"GPKG\")\n",
    "print(f\"‚úÖ Saved differences dataset to: {diff_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sous-dossiers de data: ['density_population_comune', 'Morocco', 'Morocco-Counties-shapefile']\n",
      "Fichiers dans density_population_comune: ['populaion_commune.cpg', 'populaion_commune.dbf', 'populaion_commune.prj', 'populaion_commune.shp', 'populaion_commune.shx']\n",
      "Lecture de : c:\\Users\\Ahmed\\OoklaOpenDataMorocco\\data\\density_population_comune\\populaion_commune.shp\n",
      "‚úÖ Charg√© : (1505, 10) lignes\n",
      "CRS : EPSG:4326\n",
      "Colonnes : ['Code_Commu', 'Code_Provi', 'CODE_REGIO', 'nom', 'Marocains_', 'Etrangers_', 'Populati_1', 'Menages_', 'nom_arabe', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "# V√©rifions le contenu du dossier\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "print(\"Sous-dossiers de data:\", os.listdir(data_dir))\n",
    "\n",
    "\n",
    "communes_dir = os.path.join(data_dir, \"density_population_comune\")\n",
    "print(\"Fichiers dans density_population_comune:\", os.listdir(communes_dir))\n",
    "\n",
    "# Chemin exact vers le .shp (ajustez selon le nom retourn√© ci-dessus)\n",
    "shp_path = os.path.join(communes_dir, \"populaion_commune.shp\")\n",
    "\n",
    "# Optionnel : reconstruire le .shx si besoin\n",
    "os.environ[\"SHAPE_RESTORE_SHX\"] = \"YES\"\n",
    "\n",
    "# Chargement GeoPandas\n",
    "print(\"Lecture de :\", shp_path)\n",
    "communes = gpd.read_file(shp_path)\n",
    "\n",
    "print(\"‚úÖ Charg√© :\", communes.shape, \"lignes\")\n",
    "print(\"CRS :\", communes.crs)\n",
    "print(\"Colonnes :\", communes.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir : c:\\Users\\Ahmed\\OoklaOpenDataMorocco\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Confirmez le r√©pertoire courant\n",
    "print(\"Working dir :\", os.getcwd())\n",
    "\n",
    "# 2. Cherchez r√©cursivement les .gpkg\n",
    "for root, dirs, files in os.walk(os.getcwd()):\n",
    "    for f in files:\n",
    "        if f.endswith(\".gpkg\"):\n",
    "            print(\"‚Üí Trouv√© :\", f, \"dans\", root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture de : c:\\Users\\Ahmed\\OoklaOpenDataMorocco\\data\\density_population_comune\\all_quarters_differences.gpkg\n",
      "‚úÖ Charg√© : (42997, 76) lignes\n",
      "Colonnes : ['quadkey', '2024-04-01_fixed_avg_d_kbps', '2024-04-01_fixed_avg_u_kbps', '2024-04-01_fixed_avg_lat_ms', '2024-04-01_fixed_tests', '2024-04-01_fixed_devices', '2024-07-01_fixed_avg_d_kbps', '2024-07-01_fixed_avg_u_kbps', '2024-07-01_fixed_avg_lat_ms', '2024-07-01_fixed_tests', '2024-07-01_fixed_devices', '2024-10-01_fixed_avg_d_kbps', '2024-10-01_fixed_avg_u_kbps', '2024-10-01_fixed_avg_lat_ms', '2024-10-01_fixed_tests', '2024-10-01_fixed_devices', '2024-04-01_mobile_avg_d_kbps', '2024-04-01_mobile_avg_u_kbps', '2024-04-01_mobile_avg_lat_ms', '2024-04-01_mobile_tests', '2024-04-01_mobile_devices', '2024-01-01_fixed_avg_d_kbps', '2024-01-01_fixed_avg_u_kbps', '2024-01-01_fixed_avg_lat_ms', '2024-01-01_fixed_tests', '2024-01-01_fixed_devices', '2024-07-01_mobile_avg_d_kbps', '2024-07-01_mobile_avg_u_kbps', '2024-07-01_mobile_avg_lat_ms', '2024-07-01_mobile_tests', '2024-07-01_mobile_devices', '2025-01-01_fixed_avg_d_kbps', '2025-01-01_fixed_avg_u_kbps', '2025-01-01_fixed_avg_lat_ms', '2025-01-01_fixed_tests', '2025-01-01_fixed_devices', '2024-10-01_mobile_avg_d_kbps', '2024-10-01_mobile_avg_u_kbps', '2024-10-01_mobile_avg_lat_ms', '2024-10-01_mobile_tests', '2024-10-01_mobile_devices', '2025-01-01_mobile_avg_d_kbps', '2025-01-01_mobile_avg_u_kbps', '2025-01-01_mobile_avg_lat_ms', '2025-01-01_mobile_tests', '2025-01-01_mobile_devices', '2024-01-01_mobile_avg_d_kbps', '2024-01-01_mobile_avg_u_kbps', '2024-01-01_mobile_avg_lat_ms', '2024-01-01_mobile_tests', '2024-01-01_mobile_devices', 'D_2024-04-01_M_2024-01-01_fixed_avg_d_kbps', 'D_2024-07-01_M_2024-04-01_fixed_avg_d_kbps', 'D_2024-10-01_M_2024-07-01_fixed_avg_d_kbps', 'D_2025-01-01_M_2024-10-01_fixed_avg_d_kbps', 'D_2024-04-01_M_2024-01-01_fixed_avg_u_kbps', 'D_2024-07-01_M_2024-04-01_fixed_avg_u_kbps', 'D_2024-10-01_M_2024-07-01_fixed_avg_u_kbps', 'D_2025-01-01_M_2024-10-01_fixed_avg_u_kbps', 'D_2024-04-01_M_2024-01-01_fixed_avg_lat_ms', 'D_2024-07-01_M_2024-04-01_fixed_avg_lat_ms', 'D_2024-10-01_M_2024-07-01_fixed_avg_lat_ms', 'D_2025-01-01_M_2024-10-01_fixed_avg_lat_ms', 'D_2024-04-01_M_2024-01-01_mobile_avg_d_kbps', 'D_2024-07-01_M_2024-04-01_mobile_avg_d_kbps', 'D_2024-10-01_M_2024-07-01_mobile_avg_d_kbps', 'D_2025-01-01_M_2024-10-01_mobile_avg_d_kbps', 'D_2024-04-01_M_2024-01-01_mobile_avg_u_kbps', 'D_2024-07-01_M_2024-04-01_mobile_avg_u_kbps', 'D_2024-10-01_M_2024-07-01_mobile_avg_u_kbps', 'D_2025-01-01_M_2024-10-01_mobile_avg_u_kbps', 'D_2024-04-01_M_2024-01-01_mobile_avg_lat_ms', 'D_2024-07-01_M_2024-04-01_mobile_avg_lat_ms', 'D_2024-10-01_M_2024-07-01_mobile_avg_lat_ms', 'D_2025-01-01_M_2024-10-01_mobile_avg_lat_ms', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "# 1) Construire le chemin vers le GeoPackage\n",
    "base_dir   = os.getcwd()  # c:\\Users\\Ahmed\\OoklaOpenDataMorocco\n",
    "data_dir   = os.path.join(base_dir, \"data\", \"density_population_comune\")\n",
    "gpkg_path  = os.path.join(data_dir, \"all_quarters_differences.gpkg\")\n",
    "\n",
    "print(\"Lecture de :\", gpkg_path)\n",
    "tiles = gpd.read_file(gpkg_path)\n",
    "\n",
    "print(\"‚úÖ Charg√© :\", tiles.shape, \"lignes\")\n",
    "print(\"Colonnes :\", tiles.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Jeu de donn√©es enrichi enregistr√© :\n",
      "  ‚Ä¢ GeoPackage ‚Üí data/tiles_with_density.gpkg\n",
      "  ‚Ä¢ CSV         ‚Üí data/tiles_with_density.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Chargez la table des tuiles\n",
    "tiles = gpd.read_file(\n",
    "    os.path.join(\"data\", \"density_population_comune\", \"all_quarters_differences.gpkg\")\n",
    ")\n",
    "\n",
    "# 2) Chargez la couche communes et calculez la densit√©\n",
    "communes = gpd.read_file(\n",
    "    os.path.join(\"data\", \"density_population_comune\", \"populaion_commune.shp\")\n",
    ")\n",
    "\n",
    "# (a) Passez en m√©trique pour la surface\n",
    "communes_m = communes.to_crs(epsg=3857)\n",
    "communes[\"area_km2\"] = communes_m.geometry.area / 1e6\n",
    "\n",
    "# (b) Calculez densit√© : population / km¬≤\n",
    "communes[\"densite_pop\"] = communes[\"Populati_1\"] / communes[\"area_km2\"]\n",
    "\n",
    "# (c) Remettez en EPSG:4326 pour matcher les tuiles\n",
    "communes = communes.to_crs(tiles.crs)\n",
    "\n",
    "# 3) Jointure spatiale ¬´ within ¬ª  : chaque tuile h√©rite de la densit√© de sa commune\n",
    "tiles_with_pop = gpd.sjoin(\n",
    "    tiles,\n",
    "    communes[[\"geometry\", \"densite_pop\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"\n",
    ")\n",
    "\n",
    "# 4) (Optionnel) Convertir en DataFrame pandas pour exporter sans g√©om√©trie\n",
    "df_model = pd.DataFrame(tiles_with_pop.drop(columns=\"geometry\"))\n",
    "\n",
    "# 5) Sauvegarder votre jeu de donn√©es enrichi\n",
    "#  - g√©opackage si vous voulez garder la g√©om√©trie\n",
    "tiles_with_pop.to_file(\n",
    "    os.path.join(\"data\", \"tiles_with_density.gpkg\"),\n",
    "    driver=\"GPKG\"\n",
    ")\n",
    "\n",
    "#  - ou CSV si vous ne voulez que les variables tabulaires\n",
    "df_model.to_csv(\n",
    "    os.path.join(\"data\", \"tiles_with_density.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Jeu de donn√©es enrichi enregistr√© :\")\n",
    "print(\"  ‚Ä¢ GeoPackage ‚Üí data/tiles_with_density.gpkg\")\n",
    "print(\"  ‚Ä¢ CSV         ‚Üí data/tiles_with_density.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RMSE (test) = 49070.83 kbps\n",
      "‚úÖ R√©sultats sauvegard√©s dans data/tiles_score_rf.gpkg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.ensemble       import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics        import mean_squared_error\n",
    "\n",
    "# 1Ô∏è‚É£ INPUT\n",
    "# GeoPackage enrichi avec g√©om√©trie et densit√©\n",
    "in_path = \"data/tiles_with_density.gpkg\"\n",
    "\n",
    "# 2Ô∏è‚É£ CHARGEMENT\n",
    "tiles = gpd.read_file(in_path)\n",
    "\n",
    "# 3Ô∏è‚É£ D√âFINITION DES FEATURES ET DE LA TARGET\n",
    "features = [\n",
    "    \"2024-10-01_mobile_avg_d_kbps\",               # d√©bit mobile dernier trimestre\n",
    "    \"2024-10-01_fixed_avg_d_kbps\",                # d√©bit fixe dernier trimestre\n",
    "    \"D_2024-10-01_M_2024-07-01_mobile_avg_d_kbps\", # variation mobile Q-1 vs Q-2\n",
    "    \"densite_pop\"                                 # densit√© de population\n",
    "]\n",
    "target = \"2025-01-01_mobile_avg_d_kbps\"           # d√©bit mobile √† pr√©dire\n",
    "\n",
    "# 4Ô∏è‚É£ PR√âPARATION DU JEU D‚ÄôENTRA√éNEMENT\n",
    "df = tiles[features + [target]].dropna()\n",
    "X, y = df[features], df[target]\n",
    "\n",
    "# 5Ô∏è‚É£ SPLIT TRAIN / TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£ ENTRA√éNEMENT DU RANDOM FOREST\n",
    "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7Ô∏è‚É£ √âVALUATION\n",
    "y_pred = model.predict(X_test)\n",
    "mse  = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"üìä RMSE (test) = {rmse:.2f} kbps\")\n",
    "\n",
    "# 8Ô∏è‚É£ SCORING DE TOUTES LES TUILES\n",
    "#  - pred_rf   : d√©bit pr√©dit\n",
    "#  - residu_rf : √©cart observ√© ‚àí pr√©dit\n",
    "#  - score_rf  : priorit√© = densit√© √ó (‚àír√©sidu)\n",
    "tiles[\"pred_rf\"]   = model.predict(tiles[features].fillna(0))\n",
    "tiles[\"residu_rf\"] = tiles[target] - tiles[\"pred_rf\"]\n",
    "tiles[\"score_rf\"]  = - tiles[\"residu_rf\"] * tiles[\"densite_pop\"]\n",
    "\n",
    "# 9Ô∏è‚É£ EXPORT DES R√âSULTATS\n",
    "out_path = \"data/tiles_score_rf.gpkg\"\n",
    "tiles.to_file(out_path, driver=\"GPKG\")\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s dans {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèéÔ∏è Bandwidth retenue : 230.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\Ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\Ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\geopandas\\geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GeoPackage GWR enregistr√© dans : data/tiles_score_gwr.gpkg\n"
     ]
    }
   ],
   "source": [
    "# 0Ô∏è‚É£ (Jupyter uniquement) Installer mgwr si besoin :\n",
    "# !pip install mgwr\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from mgwr.gwr    import GWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "\n",
    "# 1Ô∏è‚É£ INPUT\n",
    "in_path = \"data/tiles_with_density.gpkg\"\n",
    "tiles   = gpd.read_file(in_path)  # CRS EPSG:4326\n",
    "\n",
    "# 2Ô∏è‚É£ PROJECTION en m√©trique\n",
    "tiles_proj = tiles.to_crs(epsg=3857)\n",
    "\n",
    "# 3Ô∏è‚É£ D√âFINITIONS\n",
    "target    = \"2025-01-01_mobile_avg_d_kbps\"\n",
    "covar_den = \"densite_pop\"\n",
    "covar_dl1 = \"2024-10-01_mobile_avg_d_kbps\"\n",
    "\n",
    "# 4Ô∏è‚É£ FILTRAGE NaN\n",
    "cols_needed = [covar_den, covar_dl1, target]\n",
    "tiles_clean = tiles_proj.dropna(subset=cols_needed)\n",
    "\n",
    "# 5Ô∏è‚É£ CENTRO√èDES + COORDS\n",
    "centroids = tiles_clean.geometry.centroid\n",
    "coords    = np.vstack((centroids.x, centroids.y)).T   # shape (n,2)\n",
    "\n",
    "# 6Ô∏è‚É£ MATRICES X et y\n",
    "X = tiles_clean[[covar_den, covar_dl1]].values         # (n,2)\n",
    "y = tiles_clean[[target]].values                       # (n,1)\n",
    "\n",
    "# 7Ô∏è‚É£ RECHERCHE DE LA BANDE PASSANTE\n",
    "selector  = Sel_BW(coords, y, X)\n",
    "bandwidth = selector.search()\n",
    "print(f\"üèéÔ∏è Bandwidth retenue : {bandwidth:.2f}\")\n",
    "\n",
    "# 8Ô∏è‚É£ FIT GWR\n",
    "gwr_model   = GWR(coords, y, X, bandwidth)\n",
    "gwr_results = gwr_model.fit()\n",
    "\n",
    "# 9Ô∏è‚É£ PREDICTION EXPLICITE\n",
    "pred_res = gwr_model.predict(coords, X)\n",
    "tiles_clean[\"pred_gwr\"]   = pred_res.predictions.flatten()\n",
    "tiles_clean[\"residu_gwr\"] = tiles_clean[target] - tiles_clean[\"pred_gwr\"]\n",
    "\n",
    "# üîü SCORE DE PRIORIT√â\n",
    "tiles_clean[\"score_gwr\"] = - tiles_clean[\"residu_gwr\"] * tiles_clean[covar_den]\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ REINJECTION dans le GeoDataFrame complet\n",
    "for col in [\"pred_gwr\", \"residu_gwr\", \"score_gwr\"]:\n",
    "    tiles_proj.loc[tiles_clean.index, col] = tiles_clean[col]\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ REMISE EN CRS ORIGINE\n",
    "result = tiles_proj.to_crs(tiles.crs)\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ EXPORT\n",
    "out_path = \"data/tiles_score_gwr.gpkg\"\n",
    "result.to_file(out_path, driver=\"GPKG\")\n",
    "print(f\"‚úÖ GeoPackage GWR enregistr√© dans : {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ahmed\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è r√©sultats sauvegard√©s dans data/tiles_score_hdb.gpkg\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Contexte & d√©pendances\n",
    "#   * hdbscan pour le clustering, sklearn pour la normalisation\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "\n",
    "# 2Ô∏è‚É£ Inputs\n",
    "#   - Toujours ‚Äútiles_with_density.gpkg‚Äù\n",
    "#   - On clusterise sur :\n",
    "#       ‚Ä¢ 2024-10-01_mobile_avg_d_kbps\n",
    "#       ‚Ä¢ D_2024-10-01_M_2024-07-01_mobile_avg_d_kbps\n",
    "#       ‚Ä¢ densite_pop\n",
    "tiles = gpd.read_file(\"data/tiles_with_density.gpkg\")\n",
    "\n",
    "feat = tiles[[\n",
    "    \"2024-10-01_mobile_avg_d_kbps\",\n",
    "    \"D_2024-10-01_M_2024-07-01_mobile_avg_d_kbps\",\n",
    "    \"densite_pop\"\n",
    "]].fillna(0)\n",
    "\n",
    "# 3Ô∏è‚É£ √âtapes cl√©s\n",
    "# 3.1 Standardisation\n",
    "Xs = StandardScaler().fit_transform(feat)\n",
    "\n",
    "# 3.2 Clustering HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50)\n",
    "labels    = clusterer.fit_predict(Xs)\n",
    "tiles[\"cluster\"] = labels\n",
    "\n",
    "# 3.3 D√©finir les clusters critiques\n",
    "#    On consid√®re critiques ceux dont la densit√© est dans le top 25% et d√©bit dans le bottom 25%\n",
    "summary = tiles.groupby(\"cluster\").agg({\n",
    "    \"densite_pop\": \"mean\",\n",
    "    \"2024-10-01_mobile_avg_d_kbps\": \"mean\"\n",
    "})\n",
    "dens_high = summary[\"densite_pop\"].quantile(0.75)\n",
    "dl_low    = summary[\"2024-10-01_mobile_avg_d_kbps\"].quantile(0.25)\n",
    "top_clusters = summary[\n",
    "    (summary[\"densite_pop\"] >= dens_high) &\n",
    "    (summary[\"2024-10-01_mobile_avg_d_kbps\"] <= dl_low)\n",
    "].index.tolist()\n",
    "\n",
    "# 3.4 Score binaire : 1 si cluster critique, 0 sinon\n",
    "tiles[\"score_hdb\"] = tiles[\"cluster\"].apply(lambda c: 1 if c in top_clusters else 0)\n",
    "\n",
    "# 4Ô∏è‚É£ Outputs\n",
    "#   - Dans `tiles` :\n",
    "#       ‚Ä¢ cluster    : label HDBSCAN\n",
    "#       ‚Ä¢ score_hdb  : indicateur de zone critique\n",
    "tiles.to_file(\"data/tiles_score_hdb.gpkg\", driver=\"GPKG\")\n",
    "print(\"‚û°Ô∏è r√©sultats sauvegard√©s dans data/tiles_score_hdb.gpkg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
